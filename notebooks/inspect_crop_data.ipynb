{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Crop Training Data\n",
    "\n",
    "Inspect and visualize data loading and pre-processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "import imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "    \n",
    "# Import Mask RCNN\n",
    "from cropmask.mrcnn import utils, visualize\n",
    "from cropmask.mrcnn import model as modellib\n",
    "from cropmask.mrcnn.model import log\n",
    "\n",
    "# add module packages\n",
    "from cropmask import model_configs, datasets, preprocess\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out to reload imported modules if they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use configuation from crop_mask.py, but override\n",
    "# image resizing so we see the real sizes here\n",
    "class NoResizeConfig(model_configs.LandsatConfig):\n",
    "    IMAGE_RESIZE_MODE = \"none\"\n",
    "    \n",
    "config = NoResizeConfig(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=20):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The data should be in data/raw/*dataset-id*/train after running `python crop_mask.py preprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.ImageDataset()\n",
    "wflow = preprocess.PreprocessWorkflow(\"/home/ryan/work/CropMask_RCNN/cropmask/preprocess_config.yaml\", \n",
    "                                 \"/mnt/azureml-filestore-896933ab-f4fd-42b2-a154-0abb35dfb0b0/unpacked_landsat_downloads/032031/LT050320312005082801T1-SC20190418222350/\",\n",
    "                                 \"/mnt/azureml-filestore-896933ab-f4fd-42b2-a154-0abb35dfb0b0/external/nebraska_pivots_projected.geojson\")\n",
    "# The subset is the name of the sub-directory, such as stage1_train,\n",
    "# stage1_test, ...etc. You can also use these special values:\n",
    "#     train: loads stage1_train but excludes validation images\n",
    "#     val: loads validation images from stage1_train. For a list\n",
    "#          of validation images see crop_mask.py\n",
    "dataset.load_imagery(wflow.DATASET, subset=\"train\", image_source='landsat-5', class_name=\"center-pivot\", train_test_split_dir=wflow.RESULTS)\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples \n",
    "http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bad cps are tough to fix. Can't import tifs into qgis, opening doesn't fix all the cps. will revist if model trianing/detection is especially bad. I could seperate these into my test folder and only train on good circles. These are in the center pivot directory, need to change paths or delete this old cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as skio\n",
    "arr = skio.imread(\"/mnt/azureml-filestore-896933ab-f4fd-42b2-a154-0abb35dfb0b0/landsat-1024-cp/train/tile_4096-5632/image/tile_4096-5632.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:,:,0].astype(float)/1000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(arr.shape[-1]):\n",
    "    the_max = float(np.max(arr[:,:,i]))\n",
    "    print(len(np.unique(arr[:,:,i])))\n",
    "    arr[:,:,i] = arr[:,:,i].astype(float)/the_max\n",
    "    print(len(np.unique(arr[:,:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skio.imshow(arr[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset.image_ids, 3)\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    print(dataset.source_image_link(image_id))\n",
    "    print(mask.shape)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.image_info[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a specific image by its source ID get id from above or from train folder\n",
    "source_id = 0\n",
    "\n",
    "# Map source ID to Dataset image_id\n",
    "# Notice the prefix: it's the name given to the dataset in the Dataset class\n",
    "image_id = dataset.image_from_source_map[\"landsat-5.{}\".format(dataset.image_info[source_id]['id'])]\n",
    "\n",
    "# Load and display\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, config, image_id, use_mini_mask=False)\n",
    "image = image.astype(np.float32) # for landsat only\n",
    "image *= (255.0/image.max()) # for landsat only\n",
    "log(\"molded_image\", image)\n",
    "log(\"mask\", mask)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names,\n",
    "                            show_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats\n",
    "\n",
    "Loop through all images in the dataset and collect aggregate stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats(image_id):\n",
    "    \"\"\"Returns a dict of stats for one image.\"\"\"\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, _ = dataset.load_mask(image_id)\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "    # Sanity check\n",
    "    assert mask.shape[:2] == image.shape[:2]\n",
    "    # Return stats dict\n",
    "    return {\n",
    "        \"id\": image_id,\n",
    "        \"shape\": list(image.shape),\n",
    "        \"bbox\": [[b[2] - b[0], b[3] - b[1]]\n",
    "                 for b in bbox\n",
    "                 # Uncomment to exclude fields with 1 pixel width\n",
    "                 # or height (often on edges)\n",
    "                 if b[2] - b[0] > 1 and b[3] - b[1] > 1\n",
    "                ],\n",
    "        \"color\": np.mean(image, axis=(0, 1)),\n",
    "    }\n",
    "\n",
    "# Loop through the dataset and compute stats over multiple threads\n",
    "# This might take a few minutes\n",
    "t_start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor() as e:\n",
    "    stats = list(e.map(image_stats, dataset.image_ids))\n",
    "t_total = time.time() - t_start\n",
    "print(\"Total time: {:.1f} seconds\".format(t_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image stats\n",
    "image_shape = np.array([s['shape'] for s in stats])\n",
    "image_color = np.array([s['color'] for s in stats])\n",
    "print(\"Image Count: \", image_shape.shape[0])\n",
    "print(\"Height  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(image_shape[:, 0]), np.median(image_shape[:, 0]),\n",
    "    np.min(image_shape[:, 0]), np.max(image_shape[:, 0])))\n",
    "print(\"Width   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(image_shape[:, 1]), np.median(image_shape[:, 1]),\n",
    "    np.min(image_shape[:, 1]), np.max(image_shape[:, 1])))\n",
    "print(\"Color   mean (RGB): {:.2f} {:.2f} {:.2f}\".format(*np.mean(image_color, axis=0)))\n",
    "\n",
    "# Histograms\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].set_title(\"Height\")\n",
    "_ = ax[0].hist(image_shape[:, 0], bins=20)\n",
    "ax[1].set_title(\"Width\")\n",
    "_ = ax[1].hist(image_shape[:, 1], bins=20)\n",
    "ax[2].set_title(\"Height & Width\")\n",
    "_ = ax[2].hist2d(image_shape[:, 1], image_shape[:, 0], bins=10, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields per Image Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by image area, edited to only work with one area threshold, for loop not needed but doesn't hurt\n",
    "image_area_bins = [512**2]\n",
    "\n",
    "print(\"Fields/Image\")\n",
    "fig, ax = plt.subplots(1, len(image_area_bins), figsize=(16, 4))\n",
    "area_threshold = 0\n",
    "for i, image_area in enumerate(image_area_bins):\n",
    "    fields_per_image = np.array([len(s['bbox']) \n",
    "                                 for s in stats \n",
    "                                 if area_threshold < (s['shape'][0] * s['shape'][1]) <= image_area])\n",
    "    area_threshold = image_area\n",
    "    if len(fields_per_image) == 0:\n",
    "        print(\"For Image area <= {:4}**2: None\".format(np.sqrt(image_area)))\n",
    "        continue\n",
    "    print(\"For Image area <= {:4.0f}**2:  mean: {:.1f}  median: {:.1f}  min: {:.1f}  max: {:.1f}\".format(\n",
    "        np.sqrt(image_area), fields_per_image.mean(), np.median(fields_per_image), \n",
    "        fields_per_image.min(), fields_per_image.max()))\n",
    "    ax.set_title(\"Image Area <= {:4}**2\".format(np.sqrt(image_area)))\n",
    "    _ = ax.hist(fields_per_image, bins=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields size stats\n",
    "fig, ax = plt.subplots(1, len(image_area_bins), figsize=(16, 4))\n",
    "area_threshold = 0\n",
    "for i, image_area in enumerate(image_area_bins):\n",
    "    field_shape = np.array([\n",
    "        b\n",
    "        for s in stats if area_threshold < (s['shape'][0] * s['shape'][1]) <= image_area\n",
    "        for b in s['bbox']])\n",
    "    print(stats[0])\n",
    "    field_area = field_shape[:, 0] * field_shape[:, 1]\n",
    "    area_threshold = image_area\n",
    "\n",
    "    print(\"\\nImage Area <= {:.0f}**2 units are in pixels\".format(np.sqrt(image_area)))\n",
    "    print(\"  Total Fields: \", field_shape.shape[0])\n",
    "    print(\"  field Height. mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(field_shape[:, 0]), np.median(field_shape[:, 0]),\n",
    "        np.min(field_shape[:, 0]), np.max(field_shape[:, 0])))\n",
    "    print(\"  field Width.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(field_shape[:, 1]), np.median(field_shape[:, 1]),\n",
    "        np.min(field_shape[:, 1]), np.max(field_shape[:, 1])))\n",
    "    print(\"  field Area.   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(field_area), np.median(field_area),\n",
    "        np.min(field_area), np.max(field_area)))\n",
    "\n",
    "    # Show 2D histogram\n",
    "    _ = ax.hist2d(field_shape[:, 1], field_shape[:, 0], bins=60, cmap=\"Blues\", range = [[20,55],[20,55]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields height/width ratio\n",
    "field_aspect_ratio = field_shape[:, 0] / field_shape[:, 1]\n",
    "print(\"field Aspect Ratio.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(field_aspect_ratio), np.median(field_aspect_ratio),\n",
    "    np.min(field_aspect_ratio), np.max(field_aspect_ratio)))\n",
    "plt.figure(figsize=(15, 5))\n",
    "_ = plt.hist(field_aspect_ratio, bins=100, range=[0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Test out different augmentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of augmentations\n",
    "# http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "augmentation = iaa.Sometimes(.9,[\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.GaussianBlur(sigma=(0.1, 2.0)),\n",
    "#     iaa.AdditiveGaussianNoise(scale=0.9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the image multiple times to show augmentations\n",
    "limit = 4\n",
    "ax = get_ax(rows=2, cols=limit//2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, config, image_id, use_mini_mask=False, augment=False, augmentation=augmentation)\n",
    "    visualize.display_instances(image, bbox, mask, class_ids,\n",
    "                                dataset.class_names, ax=ax[i//2, i % 2],\n",
    "                                show_mask=False, show_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Crops\n",
    "\n",
    "Our gridded images are small (about 260x260) so we probably don't need to crop them before hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropConfig(model_configs.LandsatConfig):\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "\n",
    "crop_config = RandomCropConfig(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image multiple times to show augmentations\n",
    "limit = 4\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "ax = get_ax(rows=2, cols=limit//2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, crop_config, image_id, use_mini_mask=False)\n",
    "    visualize.display_instances(image, bbox, mask, class_ids,\n",
    "                                dataset.class_names, ax=ax[i//2, i % 2],\n",
    "                                show_mask=False, show_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Masks\n",
    "Below is the decription for the nucleus dataset. Since field size on average is about 78x78 and there are still quite a few fields that are larger, it is probably best not to optimize with mini masks, especially since our gridded images are noth that large - 256x256.\n",
    "\n",
    "Instance binary masks can get large when training with high resolution images. For example, if training with 1024x1024 image then the mask of a single instance requires 1MB of memory (Numpy uses bytes for boolean values). If an image has 100 instances then that's 100MB for the masks alone. \n",
    "\n",
    "To improve training speed, we optimize masks:\n",
    "* We store mask pixels that are inside the object bounding box, rather than a mask of the full image. Most objects are small compared to the image size, so we save space by not storing a lot of zeros around the object.\n",
    "* We resize the mask to a smaller size (e.g. 56x56). For objects that are larger than the selected size we lose a bit of accuracy. But most object annotations are not very accuracy to begin with, so this loss is negligable for most practical purposes. Thie size of the mini_mask can be set in the config class.\n",
    "\n",
    "To visualize the effect of mask resizing, and to verify the code correctness, we visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random image and mask.\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image = dataset.load_image(image_id)\n",
    "mask, class_ids = dataset.load_mask(image_id)\n",
    "original_shape = image.shape\n",
    "# Resize\n",
    "image, window, scale, padding, _ = utils.resize_image(\n",
    "    image, \n",
    "    min_dim=config.IMAGE_MIN_DIM, \n",
    "    max_dim=config.IMAGE_MAX_DIM,\n",
    "    mode=config.IMAGE_RESIZE_MODE)\n",
    "mask = utils.resize_mask(mask, scale, padding)\n",
    "# Compute Bounding box\n",
    "bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "print(\"Original shape: \", original_shape)\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fails for images with no fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"image\", image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "log(\"mask\", mask)\n",
    "\n",
    "visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.savefig(\"/home/rave/cloudy_landsat_scene_example_with_missing_label_diff_sizes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add augmentation and mask resizing.\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, augment=True, use_mini_mask=True)\n",
    "log(\"mask\", mask)\n",
    "visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = utils.expand_mask(bbox, mask, image.shape)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors\n",
    "\n",
    "For an FPN network, the anchors must be ordered in a way that makes it easy to match anchors to the output of the convolution layers that predict anchor scores and shifts. \n",
    "* Sort by pyramid level first. All anchors of the first level, then all of the second and so on. This makes it easier to separate anchors by level.\n",
    "* Within each level, sort anchors by feature map processing sequence. Typically, a convolution layer processes a feature map starting from top-left and moving right row by row. \n",
    "* For each feature map cell, pick any sorting order for the anchors of different ratios. Here we match the order of ratios passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functionality has been moved to mrcnn/visualize.py, keeping for now for interactive plotting\n",
    "import skimage.io as skio\n",
    "from skimage import exposure\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "def reorder_to_brg(image):\n",
    "    '''reorders landsat bands ordered like RGB for off/onseason\n",
    "    to blue, red, green for imshow\n",
    "    '''\n",
    "    blue = normalize(image[:,:,0])\n",
    "    green = normalize(image[:,:,1])\n",
    "    red = normalize(image[:,:,2])\n",
    "    return np.stack([red, green, blue], axis=-1)\n",
    "\n",
    "def percentile_rescale(arr):\n",
    "    '''\n",
    "    Rescales and applies other exposure functions to improve image vis. \n",
    "    http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.rescale_intensity\n",
    "    '''\n",
    "    rescaled_arr = np.zeros_like(arr)\n",
    "    for i in range(0,arr.shape[-1]):\n",
    "        val_range = (np.percentile(arr[:,:,i], 2), np.percentile(arr[:,:,i], 98))\n",
    "        rescaled_channel = exposure.rescale_intensity(arr[:,:,i], val_range)\n",
    "        rescaled_arr[:,:,i] = rescaled_channel\n",
    "        #rescaled_arr= exposure.adjust_gamma(rescaled_arr, gamma=1) #adjust from 1 either way\n",
    "#     rescaled_arr= exposure.adjust_sigmoid(rescaled_arr, cutoff=.50) #adjust from .5 either way  \n",
    "    return rescaled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs.LandsatConfig.RPN_ANCHOR_SCALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visualize anchors of one cell at the center of the feature map\n",
    "\n",
    "# Load and display random image\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(dataset, model_configs.LandsatConfig, image_id)\n",
    "\n",
    "# Generate Anchors\n",
    "config= model_configs.LandsatConfig\n",
    "backbone_shapes = modellib.compute_backbone_shapes(config, image.shape)\n",
    "anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n",
    "                                          config.RPN_ANCHOR_RATIOS,\n",
    "                                          backbone_shapes,\n",
    "                                          config.BACKBONE_STRIDES, \n",
    "                                          config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "# Print summary of anchors\n",
    "num_levels = len(backbone_shapes)\n",
    "anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Count: \", anchors.shape[0])\n",
    "print(\"Scales: \", config.RPN_ANCHOR_SCALES)\n",
    "print(\"ratios: \", config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Anchors per Cell: \", anchors_per_cell)\n",
    "print(\"Levels: \", num_levels)\n",
    "anchors_per_level = []\n",
    "for l in range(num_levels):\n",
    "    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n",
    "    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n",
    "    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))\n",
    "\n",
    "# Display\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "image = reorder_to_brg(image)\n",
    "# image = exposure.equalize_adapthist(image, clip_limit=0.0055)\n",
    "image = percentile_rescale(image)\n",
    "ax.imshow(image, vmin=5)\n",
    "levels = len(backbone_shapes)\n",
    "\n",
    "for level in range(levels):\n",
    "    colors = visualize.random_colors(levels)\n",
    "    # Compute the index of the anchors at the center of the image\n",
    "    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n",
    "    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n",
    "    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n",
    "                                                                backbone_shapes[level]))\n",
    "    center_cell = backbone_shapes[level] // 2\n",
    "    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n",
    "    level_center = center_cell_index * anchors_per_cell \n",
    "    center_anchor = anchors_per_cell * (\n",
    "        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n",
    "        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n",
    "    level_center = int(center_anchor)\n",
    "\n",
    "    # Draw anchors. Brightness show the order in the array, dark to bright.\n",
    "    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n",
    "        y1, x1, y2, x2 = rect\n",
    "        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n",
    "                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n",
    "        ax.add_patch(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create data generator\n",
    "random_rois = 2000\n",
    "g = modellib.data_generator(\n",
    "    dataset, crop_config, shuffle=True, random_rois=random_rois, \n",
    "    batch_size=4,\n",
    "    detection_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to run the generator through a lot of images\n",
    "# to catch rare errors\n",
    "# for i in range(1000):\n",
    "#     print(i)\n",
    "#     _, _ = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Next Image\n",
    "if random_rois:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n",
    "    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n",
    "    \n",
    "    log(\"rois\", rois)\n",
    "    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n",
    "    log(\"mrcnn_bbox\", mrcnn_bbox)\n",
    "    log(\"mrcnn_mask\", mrcnn_mask)\n",
    "else:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n",
    "    \n",
    "log(\"gt_class_ids\", gt_class_ids)\n",
    "log(\"gt_boxes\", gt_boxes)\n",
    "log(\"gt_masks\", gt_masks)\n",
    "log(\"rpn_match\", rpn_match, )\n",
    "log(\"rpn_bbox\", rpn_bbox)\n",
    "image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "\n",
    "# Remove the last dim in mrcnn_class_ids. It's only added\n",
    "# to satisfy Keras restriction on target shape.\n",
    "mrcnn_class_ids = mrcnn_class_ids[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not showing true color for some reason, due to normalization in cell above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = 0\n",
    "\n",
    "# Restore original image (reverse normalization)\n",
    "sample_image = modellib.unmold_image(normalized_images[b], config)\n",
    "\n",
    "# Compute anchor shifts.\n",
    "indices = np.where(rpn_match[b] == 1)[0]\n",
    "refined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n",
    "log(\"anchors\", anchors)\n",
    "log(\"refined_anchors\", refined_anchors)\n",
    "\n",
    "# Get list of positive anchors\n",
    "positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n",
    "print(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\n",
    "negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n",
    "print(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\n",
    "neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n",
    "print(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n",
    "\n",
    "# ROI breakdown by class\n",
    "for c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n",
    "    if n:\n",
    "        print(\"{:23}: {}\".format(c[:20], n))\n",
    "\n",
    "# Show positive anchors\n",
    "fig, ax = plt.subplots(1, figsize=(16, 16))\n",
    "img = visualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n",
    "                     refined_boxes=refined_anchors, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show negative anchors\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[negative_anchor_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show neutral anchors. They don't contribute to training.\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[np.random.choice(neutral_anchor_ids, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROIs\n",
    "\n",
    "Typically, the RPN network generates region proposals (a.k.a. Regions of Interest, or ROIs). The data generator has the ability to generate proposals as well for illustration and testing purposes. These are controlled by the `random_rois` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Class aware bboxes\n",
    "    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n",
    "\n",
    "    # Refined ROIs\n",
    "    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n",
    "\n",
    "    # Class aware masks\n",
    "    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n",
    "\n",
    "    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n",
    "    \n",
    "    # Any repeated ROIs?\n",
    "    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n",
    "    _, idx = np.unique(rows, return_index=True)\n",
    "    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Dispalay ROIs and corresponding masks and bounding boxes\n",
    "    ids = random.sample(range(rois.shape[1]), 8)\n",
    "\n",
    "    images = []\n",
    "    titles = []\n",
    "    for i in ids:\n",
    "        image = visualize.draw_box(sample_image.copy(), rois[b,i,:4].astype(np.int32), [255, 0, 0])\n",
    "        image = visualize.draw_box(image, refined_rois[i].astype(np.int64), [0, 255, 0])\n",
    "        images.append(image)\n",
    "        titles.append(\"ROI {}\".format(i))\n",
    "        images.append(mask_specific[i] * 255)\n",
    "        titles.append(dataset.class_names[mrcnn_class_ids[b,i]][:20])\n",
    "\n",
    "    display_images(images, titles, cols=4, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check ratio of positive ROIs in a set of images.\n",
    "if random_rois:\n",
    "    limit = 10\n",
    "    temp_g = modellib.data_generator(\n",
    "        dataset, crop_config, shuffle=True, random_rois=10000, \n",
    "        batch_size=1, detection_targets=True)\n",
    "    total = 0\n",
    "    for i in range(limit):\n",
    "        _, [ids, _, _] = next(temp_g)\n",
    "        positive_rois = np.sum(ids[0] > 0)\n",
    "        total += positive_rois\n",
    "        print(\"{:5} {:5.2f}\".format(positive_rois, positive_rois/ids.shape[1]))\n",
    "    print(\"Average percent: {:.2f}\".format(total/(limit*ids.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
